{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "from library import w2v_modeling as wm, utils_wt as uwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Start building Word2Vec models...\nThe length of the english to indonesia training dictionary is 5000\nThe length of the english to indonesia test dictionary is 1500\nModels have been built and stored into model directory!\n"
    }
   ],
   "source": [
    "wm.build_w2v_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_subset = pickle.load(open(\"model/en_embeddings.p\", \"rb\"))\n",
    "id_embeddings_subset = pickle.load(open(\"model/id_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The length of the English to Indonesian training dictionary is 5000\nThe length of the English to Indonesian test dictionary is 5000\n"
    }
   ],
   "source": [
    "en_id_train = uwt.get_dict_en_id('RAW/train_test/en-id.train.txt')\n",
    "print('The length of the English to Indonesian training dictionary is', len(en_id_train))\n",
    "en_id_test = uwt.get_dict_en_id('RAW/train_test/en-id.test.txt')\n",
    "print('The length of the English to Indonesian test dictionary is', len(en_id_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = uwt.get_matrices(en_id_train, id_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss at iteration 0 is: 989.8128\nloss at iteration 25 is: 87.0141\nloss at iteration 50 is: 30.7166\nloss at iteration 75 is: 19.0784\nloss at iteration 100 is: 15.7499\nloss at iteration 125 is: 14.5854\nloss at iteration 150 is: 14.1179\nloss at iteration 175 is: 13.9113\nloss at iteration 200 is: 13.8134\nloss at iteration 225 is: 13.7644\nloss at iteration 250 is: 13.7387\nloss at iteration 275 is: 13.7249\nloss at iteration 300 is: 13.7171\nloss at iteration 325 is: 13.7126\nloss at iteration 350 is: 13.7100\nloss at iteration 375 is: 13.7084\n"
    }
   ],
   "source": [
    "R_train = uwt.align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = uwt.get_matrices(en_id_test, id_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# MODEL ACCURACY\n",
    "\n",
    "$$\\text{accuracy}=\\frac{\\#(\\text{correct predictions})}{\\#(\\text{total predictions})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "accuracy on test set is 47.869%\n"
    }
   ],
   "source": [
    "acc = uwt.test_vocabulary(X_test, Y_test, R_train)\n",
    "print(\"accuracy on test set is {:.3f}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model managed to translate words from one language to another language with almost 48% accuracy by using basic linear algebra and learning a mapping of words from one English to Indonesia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# FORMULAS EXPLANATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Generate embedding and transform matrices\n",
    "\n",
    "- `get_matrices` function will takes the loaded data and returns matrices `X` and `Y`\n",
    "- Matrix `X` and matrix `Y`, where each row in X is the word embedding for an english word, and the same row in Y is the word embedding for the Indonesian version of that English word.\n",
    "- Use the `en_id` dictionary to ensure that the ith row in the `X` matrix corresponds to the ith row in the `Y` matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Compute the loss\n",
    "\n",
    "* The loss function will be squared Frobenius norm of the difference between matrix and its approximation, divided by the number of training examples $m$.\n",
    "* Its formula is: $$ L(X, Y, R)=\\frac{1}{m}\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left( a_{i j} \\right)^{2}$$\n",
    "* This formula is applied in `compute_loss()` function\n",
    "* Compute the approximation of `Y` by matrix multiplying `X` and `R`\n",
    "* Compute difference `XR - Y`\n",
    "* Compute the squared Frobenius norm of the difference and divide it by $m$\n",
    "\n",
    "where $a_{i j}$ is value in $i$th row and $j$th column of the matrix $\\mathbf{XR}-\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Compute the gradient of loss to transform matrix R\n",
    "\n",
    "* The formula for the gradient of the loss function $ùêø(ùëã,ùëå,ùëÖ)$ is: $$\\frac{d}{dR}ùêø(ùëã,ùëå,ùëÖ)=\\frac{d}{dR}\\Big(\\frac{1}{m}\\| X R -Y\\|_{F}^{2}\\Big) = \\frac{2}{m}X^{T} (X R - Y)$$\n",
    "* Calculate the gradient of the loss with respect to transform matrix `R`.\n",
    "* The gradient is a matrix that encodes how much a small change in `R` affect the change in the loss function.\n",
    "* The gradient gives us the direction in which we should decrease `R` to minimize the loss\n",
    "* $m$ is the number of training examples (number of rows in $X$)\n",
    "* This formula is applied into `compute_gradient()` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Find the optimal R with Gradient Descent Algorithm\n",
    "\n",
    "* [Gradient descent](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html) is an iterative algorithm which is used in searching for the optimum of the function\n",
    "* Earlier, we've mentioned that the gradient of the loss with respect to the matrix encodes how much a tiny change in some coordinate of that matrix affect the change of loss function\n",
    "* Gradient descent uses that information to iteratively change matrix `R` until we reach a point where the loss is minimized\n",
    "* Calculate gradient $g$ of the loss with respect to the matrix $R$.\n",
    "* Update $R$ with the formula: $$R_{\\text{new}}= R_{\\text{old}}-\\alpha g$$\n",
    "* Where $\\alpha$ is the learning rate, which is a scalar\n",
    "* The learning rate or \"step size\" $\\alpha$ is a coefficient which decides how much we want to change $R$ in each step\n",
    "* If we change $R$ too much, we could skip the optimum by taking too large of a step\n",
    "* If we make only small changes to $R$, we will need many steps to reach the optimum\n",
    "* Learning rate $\\alpha$ is used to control those changes\n",
    "* Values of $\\alpha$ are chosen depending on the problem\n",
    "\n",
    "Using the training set, the transformation matrix $\\mathbf{R}$ can be found by calling the function `align_embeddings()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Test the translation using _K-Nearest Neighbors Algorithm_ with _Cosine Similarity_\n",
    "\n",
    "* Since we're approximating the translation function from English to Indonesia embeddings by a linear transformation matrix $\\mathbf{R}$, most of the time we won't get the exact embedding of a Indonesia word when we transform embedding $\\mathbf{e}$ of some particular English word into the Indonesia embedding space. \n",
    "* This is where $k$-NN becomes really useful! By using $1$-NN with $\\mathbf{eR}$ as input, we can search for an embedding $\\mathbf{f}$ (as a row) in the matrix $\\mathbf{Y}$ which is the closest to the transformed vector $\\mathbf{eR}$\n",
    "* This formula is applied in `nearest_neighbors()` function\n",
    "<br><br><br>\n",
    "Cosine similarity between vectors $u$ and $v$ calculated as the cosine of the angle between them.\n",
    "The formula is $$\\cos(u,v)=\\frac{u\\cdot v}{\\left\\|u\\right\\|\\left\\|v\\right\\|}$$\n",
    "* $\\cos(u,v)$ = $1$ when $u$ and $v$ lie on the same line and have the same direction\n",
    "* $\\cos(u,v)$ is $-1$ when they have exactly opposite directions\n",
    "* $\\cos(u,v)$ is $0$ when the vectors are orthogonal (perpendicular) to each other\n",
    "* This formula is applied in `cosine_similarity()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}